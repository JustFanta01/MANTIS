{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2.bgsegm\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from skimage import img_as_ubyte, filters\n",
    "from skimage.morphology import closing, square, remove_small_objects #, flood_fill\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingType(Enum):\n",
    "    DENOISE = 1\n",
    "    MOG = 2\n",
    "    TEMPORAL_MEDIAN = 3\n",
    "    COMBINED = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Video FPS: 12\n",
      "[I] Video Total frame count: 503\n",
      "[I] Video Length: 41.916666666666664\n",
      "[I] Video Frame Width: 320\n",
      "[I] Video Frame height: 240\n"
     ]
    }
   ],
   "source": [
    "video_path = \"../video/rilevamento-intrusioni-video.wm\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "assert cap.isOpened(), \"Not opened!\"\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "total_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "length = total_frame_count / fps\n",
    "\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(f\"[I] Video FPS: {fps}\")\n",
    "print(f\"[I] Video Total frame count: {total_frame_count}\")\n",
    "print(f\"[I] Video Length: {length}\")\n",
    "print(f\"[I] Video Frame Width: {width}\")\n",
    "print(f\"[I] Video Frame height: {height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(frame, type):\n",
    "    \"\"\"Apply all the preprocessing steps to a copy of the passed ``frame``.\n",
    "\n",
    "    Applies a series of linear and non-linear filters base on the ``type`` passed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame : ndarray\n",
    "        Grayscale input image.\n",
    "    type : ProcessingType\n",
    "        Instance of ProcessingType, it indicates which type of preprocessing will be applied.\n",
    "        - ``ProcessingType.DENOISE``\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    processed : ndarray\n",
    "        A processed copy grayscale image.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(type, ProcessingType), \"type must be an instance of ProcessingType (Enum)\"\n",
    "    output = frame.copy()\n",
    "    \n",
    "    if type == ProcessingType.DENOISE:\n",
    "        # output = cv2.medianBlur(output, 5)\n",
    "        # output = cv2.GaussianBlur(output, (5, 5), 2)\n",
    "        output = cv2.bilateralFilter(output, 3, 75, 75)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"ProcessingType.type not found\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_approxPoly(frame, epsilon = 1.0):\n",
    "    \"\"\"Use approxPoly() to redraw the contour with less vertices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame : ndarray\n",
    "        Foreground mask input image.\n",
    "    epsilon : float\n",
    "        Approximation accuracy. This is the maximum distance between the original curve and its approximation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    processed : ndarray\n",
    "        A processed copy the input image.\n",
    "    \"\"\"\n",
    "    \n",
    "    contours, _ = cv2.findContours(frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    contours_poly = [cv2.approxPolyDP(contour, epsilon, True) for contour in contours]\n",
    "    mask = np.zeros(frame.shape, np.uint8)\n",
    "    approx_poly = cv2.drawContours(mask, contours_poly, -1, (255,255,255), cv2.FILLED)\n",
    "    \n",
    "    return approx_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(fgmask, type):\n",
    "    \"\"\"Apply, to a copy of the passed ``fgmask``, a pipeline of filters and morphological operators to improve the segmentation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fgmask : ndarray\n",
    "        Foreground mask input image.\n",
    "    type : ProcessingType\n",
    "        Instance of ProcessingType, it indicates which type of preprocessing will be applied.\n",
    "        - ``ProcessingType.MOG``\n",
    "        - ``ProcessingType.TEMPORAL_MEDIAN``\n",
    "        - ``ProcessingType.COMBINED``\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    processed : ndarray\n",
    "        A processed copy the input image.\n",
    "    \"\"\"\n",
    "    assert isinstance(type, ProcessingType), \"type must be an instance of ProcessingType (Enum)\"\n",
    "    output = fgmask.copy()\n",
    "    element = np.ones((3,3), np.uint8)\n",
    "\n",
    "    if type == ProcessingType.MOG:\n",
    "        output = create_mask_from_approxPoly(output)\n",
    "        output = cv2.medianBlur(output, 3)\n",
    "        output = create_mask_from_approxPoly(output)\n",
    "        output = cv2.dilate(output, element, iterations=1)\n",
    "        output = cv2.morphologyEx(output, cv2.MORPH_CLOSE, element, iterations=1)        \n",
    "        # output = flood_fill(output, (1,1), 127, connectivity=25)\n",
    "        output = create_mask_from_approxPoly(output)\n",
    "\n",
    "    elif type == ProcessingType.TEMPORAL_MEDIAN:\n",
    "        output = create_mask_from_approxPoly(output)\n",
    "        output = cv2.medianBlur(output, 5)\n",
    "        output = create_mask_from_approxPoly(output)\n",
    "        output = cv2.morphologyEx(output, cv2.MORPH_CLOSE, element, iterations=2)\n",
    "        output = create_mask_from_approxPoly(output)\n",
    "        # output = cv2.medianBlur(output, 3)\n",
    "\n",
    "    elif type == ProcessingType.COMBINED:\n",
    "        element = np.ones((5,5), np.uint8)\n",
    "        output = cv2.dilate(output, element, iterations=1)\n",
    "        output = cv2.morphologyEx(output, cv2.MORPH_CLOSE, element, iterations=2)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"ProcessingType.type not found\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_learning_rate(frame_count, history_length, dynamic = True, bias = 0):\n",
    "    \"\"\" Computes the learning rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame_count : int \n",
    "        Current frame number\n",
    "    history_lenght : int\n",
    "        Number of frames in buffer\n",
    "    dynamic : boolean\n",
    "    bias : float\n",
    "        Constant added to the computed learning rate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    learning_rate : float\n",
    "        the learning rate to use\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # opencv/opencv/modules/video/src/bgfg_gaussmix2.cpp  #871\n",
    "    # 1./std::min( 2*nframes, history )\n",
    "\n",
    "    assert frame_count > 0, \"Frame Count must be greater than zero\"\n",
    "    if dynamic and frame_count < history_length:\n",
    "        return (1 / frame_count) + bias\n",
    "    \n",
    "\n",
    "    # const double alpha1 = 1.0f - learningRate;\n",
    "    # float weight = alpha1*gmm[mode].weight + prune; //need only weight if fit is found\n",
    "\n",
    "    return (1 / history_length) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_subtraction(frame, background, adaptive, args):\n",
    "    \"\"\" Computes the absolute difference between the frame and the background then applies a threshold to segment the\n",
    "    background and the foreground.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    frame : ndarray\n",
    "        image, current frame\n",
    "        \n",
    "    background : ndarray\n",
    "        image, estimated reference frame\n",
    "\n",
    "    adaptive : boolean\n",
    "        ``True``: use adaptive thresholding, ``False`` use hysteresis thresholding\n",
    "    \n",
    "    args : Tuple\n",
    "        It contains two types of arguments based on the ``adaptive`` flag\\\\\n",
    "        If ``adaptive``:\n",
    "        - C : float\n",
    "            Constant subtracted from the mean or weighted mean\n",
    "        - block_size : int\n",
    "            Neighbourhood size\n",
    "        \n",
    "        else:\n",
    "        - threshold_low : int \n",
    "        - threshold_high : int\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    fgmask : ndarray\n",
    "        binary image with the foreground white (255).\n",
    "    \"\"\"\n",
    "    diff = cv2.absdiff(frame, background)\n",
    "\n",
    "    if adaptive:\n",
    "        (C, block_size) = args\n",
    "        fgmask = cv2.adaptiveThreshold(diff, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, block_size, C)\n",
    "    else:\n",
    "        (t_low, t_high) = args\n",
    "        hysteresis = filters.apply_hysteresis_threshold(diff, t_low, t_high)\n",
    "        fgmask = img_as_ubyte(hysteresis)\n",
    "\n",
    "        # fgmask = combine_fgmask_with_edges(fgmask, diff_edges)\n",
    "\n",
    "    return fgmask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roi_foreground(fgmask):\n",
    "    \"\"\" Computes the absolute a rectangular ROI containing the foreground mask.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fgmask : ndarray\n",
    "        foreground mask\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : Tuple\n",
    "    - found : boolean\n",
    "    - coordinates : Tuple\n",
    "        (x_min, x_max, y_min, y_max)\n",
    "    \"\"\"\n",
    "\n",
    "    # many dilate and closing to create a bigger figure for the person and make it easier to compute the roi\n",
    "    kernel = np.ones((7,7),np.uint8)\n",
    "    dilate = cv2.dilate(fgmask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5)), iterations=1)\n",
    "    bw = cv2.morphologyEx(dilate, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    #erode = cv2.erode(bw.astype(np.uint8) * 255, None, iterations=3)\n",
    "    erode = cv2.medianBlur(bw, 3)\n",
    "    dilated = cv2.dilate(erode, None, iterations=3)\n",
    "\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    real_countours = []\n",
    "    for contour in contours:\n",
    "        # (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        if cv2.contourArea(contour) > 300:\n",
    "            real_countours.append(contour)\n",
    "        \n",
    "    \n",
    "    if len(real_countours) == 0:\n",
    "        return False, None\n",
    "    \n",
    "    # Define the ROI \n",
    "    x_min = min([np.min(cnt[:, :, 0]) for cnt in real_countours])\n",
    "    x_max = max([np.max(cnt[:, :, 0]) for cnt in real_countours])\n",
    "    y_min = min([np.min(cnt[:, :, 1]) for cnt in real_countours])\n",
    "    y_max = max([np.max(cnt[:, :, 1]) for cnt in real_countours])\n",
    "\n",
    "    padding = 20\n",
    "    x_min = max(x_min - padding, 0)\n",
    "    x_max = min(x_max + padding, width)\n",
    "    y_min = max(y_min - padding, 0)\n",
    "    y_max = min(y_max + padding, height)\n",
    "\n",
    "    return True, (x_min, x_max, y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_edges(frame1, frame2, isFrame1Binary = False, isFrame2Binary = False):\n",
    "    \"\"\"Computes the difference of the egdes of the frame1 and egdes of the frame2.\n",
    "    If the frame is binary, it will compute the edges using the morphological operator erode.\n",
    "    Instead, if the frame is grayscale, it will compute the edges using the Canny algorithm.\n",
    "    It will remove from frame1 the edges also founded in frame2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame1 : ndarray\n",
    "    frame2 : ndarray\n",
    "    isFrame1Binary : boolean\n",
    "        ``True``: Frame 1 is binary\n",
    "    isFrame2Binary : boolean\n",
    "        ``True``: Frame 2 is binary\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    img : ndarray\n",
    "        binary image with differences of edges\n",
    "    \"\"\"\n",
    "    upthresh = 150\n",
    "    lothresh = upthresh // 2\n",
    "\n",
    "    # frame2 = preprocessing(frame2, ProcessingType.EDGES)\n",
    "    if not isFrame2Binary:\n",
    "        edge_frame2 = cv2.Canny(frame2, lothresh, upthresh)\n",
    "    else:\n",
    "        diff_eroded = cv2.erode(frame2, None, iterations=1)\n",
    "        edge_frame2 =  frame2 - diff_eroded\n",
    "\n",
    "    # enlarge frame2 edges to better cancel out with the ones of the frame1\n",
    "    # elem = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "    # edge_background = cv2.dilate(edge_background, elem, iterations=1)\n",
    "    \n",
    "    # frame1 = preprocessing(frame1, ProcessingType.EDGES)\n",
    "    if not isFrame1Binary:\n",
    "        edge_frame1 = cv2.Canny(frame1, lothresh, upthresh)\n",
    "    else:\n",
    "        diff_eroded = cv2.erode(frame1, None, iterations=1)\n",
    "        edge_frame1 =  frame1 - diff_eroded\n",
    "    \n",
    "    # edge_diff will contain the edges that are present in the frame1 but not in the frame2\n",
    "    # frame1  frame2       output\n",
    "    #      0       0  -->       0\n",
    "    #    255       0  -->     255\n",
    "    #      0     255  -->       0\n",
    "    #    255     255  -->       0\n",
    "    edge_diff = edge_frame1 - np.min([edge_frame1, edge_frame2], axis=0)\n",
    "\n",
    "    processed = remove_small_objects(edge_diff.astype(bool), min_size=15, connectivity=2).astype(int)\n",
    "\n",
    "    # black out pixels\n",
    "    mask_x, mask_y = np.where(processed == 0)\n",
    "    edge_diff[mask_x, mask_y] = 0\n",
    "    \n",
    "    return edge_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_components(labels_diff, labels_comb):\n",
    "    \"\"\"Color the different blobs found by the ConnectedComponents Analysis.\n",
    "    This method is NOT general purpose and it will color the blobs according to the groundtruth colors.\n",
    "    - red : person\n",
    "    - green : removed book\n",
    "    - blue : added book\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels_diff : ndarray\n",
    "    labels_comb : ndarray\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    img : ndarray\n",
    "        binary image with differences of edges\n",
    "    \"\"\"\n",
    "        \n",
    "    # We separate the labeling for static objects (books) and dynamic objects (person)\n",
    "    # to maintain color consistency for all elements across different frames.\n",
    "    colors_statics = [(0,255,0), (255,0,0), (80,80,80), (0, 233, 0), (0,0,255)]\n",
    "    used_statics = [False, False, False, False, False] # Track used colors for static objects\n",
    "    \n",
    "    colors_dynamics = [(0,0,255), (0,255,0), (255,0,0), (254,0,0), (0, 80, 0)]\n",
    "    used_dynamics = [False, False, False, False, False] # Track used colors for dynamic objects\n",
    "\n",
    "    unique_labels_diff = np.unique(labels_diff)\n",
    "    unique_labels_comb = np.unique(labels_comb)\n",
    "\n",
    "    # Create an output image initialized to black (empty background)\n",
    "    labeled_img = np.zeros((*labels_comb.shape, 3), dtype=np.uint8)\n",
    "    color_index = 0\n",
    "\n",
    "    # labeling the static elements\n",
    "    for label in unique_labels_diff:\n",
    "        if label == 0:\n",
    "            continue  # Ignore the background\n",
    "        labeled_img[labels_diff == label] = colors_statics[color_index]\n",
    "        used_statics[color_index] = True # used to avoid using the same color for different elements\n",
    "        color_index += 1\n",
    "    \n",
    "    # labeling the dynamic elements\n",
    "    for label in unique_labels_comb:\n",
    "        if label == 0:\n",
    "            continue  # Ignore the background\n",
    "\n",
    "        # Find available colors that haven't been used for dynamic objects\n",
    "        free_indexes = [i for i in range(len(used_dynamics)) if used_dynamics[i] == False]\n",
    "        color_index = free_indexes[0]\n",
    "\n",
    "        # Ensure dynamic objects do not reuse colors assigned to static objects\n",
    "        for pos, i in enumerate(free_indexes):\n",
    "            found = False\n",
    "            for j in range(len(colors_statics)):\n",
    "                if colors_statics[j] == colors_dynamics[i] and used_statics[j] == False:\n",
    "                    color_index = i\n",
    "                    found = True\n",
    "                    break\n",
    "                elif colors_statics[j] == colors_dynamics[i] and used_statics[j] == True:\n",
    "                    color_index = free_indexes[pos+1]\n",
    "            if found == True:\n",
    "                break\n",
    "\n",
    "        # Apply the selected color to the corresponding region\n",
    "        labeled_img[labels_comb == label] = colors_dynamics[color_index]\n",
    "        used_dynamics[color_index] = True\n",
    "  \n",
    "    return labeled_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_contours(input_mask, min_area = 200):\n",
    "    \"\"\" Process the contours found in the input mask.\n",
    "    It will remove the contours with an area less than the min_area.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_mask : ndarray\n",
    "        binary image with the foreground white.\n",
    "    min_area : int\n",
    "        minimum area to consider a contour as valid\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filled_mask : ndarray\n",
    "        binary image with the filled contours    \n",
    "    \"\"\"\n",
    "\n",
    "    mask = np.zeros(input_mask.shape, np.uint8)\n",
    "    contours, _ = cv2.findContours(input_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    real_countours = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > min_area:\n",
    "            real_countours.append(contour)\n",
    "    \n",
    "    filled_mask = cv2.drawContours(mask, real_countours, -1, (255,255,255), cv2.FILLED)\n",
    "\n",
    "    return filled_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phantoms(contours, frame, background, diff):\n",
    "    \"\"\" Find the false objects in the scene.\n",
    "    It will compare the edges of the contours with the edges of the background.\n",
    "    If the edges of the contours are not present in the background, it will be considered a phantom.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    contours : ndarray\n",
    "        contours found in the scene\n",
    "    frame : ndarray\n",
    "        current frame\n",
    "    background : ndarray\n",
    "        background frame\n",
    "    diff : ndarray\n",
    "        mask of the long-term elements of the scene\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    phantom : boolean\n",
    "        True if the object is a phantom, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the bounding box around the contour\n",
    "    rect = cv2.minAreaRect(contours)\n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.int32(box)\n",
    "    box = (box[0], box[2])\n",
    "\n",
    "    # Calculate the minimum and maximum x and y coordinates for the region of interest (ROI) with padding.\n",
    "    padding = 5\n",
    "    x_min = max(min(box[0][0], box[1][0]) - padding, 0)\n",
    "    x_max = min(max(box[0][0], box[1][0]) + padding, width)\n",
    "    y_min = max(min(box[0][1], box[1][1]) - padding, 0)\n",
    "    y_max = min(max(box[0][1], box[1][1]) + padding, height)\n",
    "\n",
    "    # Extract the relevant region from the frame and background based on the bounding box and compute the Canny edge differences between the frame and background \n",
    "    # in the selected ROI, then count the number of edge pixels (white pixels).\n",
    "    presence_in_frame = diff_edges(frame[y_min:y_max, x_min:x_max], background[y_min:y_max, x_min:x_max])\n",
    "    presence_in_background_1 = diff_edges(background[y_min:y_max, x_min:x_max], frame[y_min:y_max, x_min:x_max])\n",
    "    count_presence_in_frame = np.sum(presence_in_frame == 255)\n",
    "    count_presence_in_background_1 = np.sum(presence_in_background_1 == 255)\n",
    "\n",
    "    # Looking at a long term window, the background will change and absorb those elements that where inserted/removed. For that reason, the previous method will not work\n",
    "    # anymore. We have computed the mask containing the long-term elements of the scene, so we will use it to compare the edges of the contours with the edges of the background, \n",
    "    # like that it will be possible for us to continue to detect \"phantom\" objects.\n",
    "     \n",
    "    # Extract the relevant region from the diff image and compute the edge differences between the diff image and the background in the selected ROI,\n",
    "    # then count the number of edge pixels (white pixels).\n",
    "    presence_in_diff = diff_edges(diff[y_min:y_max, x_min:x_max], background[y_min:y_max, x_min:x_max], isFrame1Binary=True)\n",
    "    presence_in_background_2 = diff_edges(background[y_min:y_max, x_min:x_max], diff[y_min:y_max, x_min:x_max], isFrame2Binary=True)\n",
    "    count_presence_in_diff = np.sum(presence_in_diff == 255)\n",
    "    count_presence_in_background_2 = np.sum(presence_in_background_2 == 255)\n",
    "\n",
    "    # If no edges are found in both images, return -1 to indicate no significant difference.\n",
    "    if count_presence_in_frame == 0 and count_presence_in_background_1 == 0 and count_presence_in_diff == 0 and count_presence_in_background_2 == 0:\n",
    "        return -1\n",
    "\n",
    "    # Determine whether the object is a \"phantom\" (false object) based on the comparison of edge counts.   \n",
    "    # First condition: shorter time window\n",
    "    # Second condition: longer time window   \n",
    "    phantom = (count_presence_in_frame <= count_presence_in_background_1) or (count_presence_in_diff >= count_presence_in_background_2)\n",
    "\n",
    "    return phantom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_background_ratio(background_ratio, history, learning_rate = -1):\n",
    "    \"\"\" Calculates the number of frames required for a static object to be considered part of the background \"\"\"\n",
    "    return math.log(background_ratio) / math.log(1 - 1/history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# |           CONFIG            |\n",
    "# -------------------------------\n",
    "BLACK = 0\n",
    "SHOW_GUI = False\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "file_output = 'output.csv'\n",
    "\n",
    "# -------------------------------\n",
    "# |      HYPERPARAMETERS        |\n",
    "# -------------------------------\n",
    "FRAME_BUFFERED_PER_SECOND = 2                   # 2 images added each second to frame buffer\n",
    "MAX_HISTORY = 3 * fps                           # 3x12 frames stored in the \"circular buffer\"\n",
    "SKIP_FRAMES = fps // FRAME_BUFFERED_PER_SECOND\n",
    "\n",
    "STATIC_THRESHOLD = 40                           # used in the absolute difference\n",
    "STATIC_THRESHOLD_HYSTERESIS = 30\n",
    "\n",
    "LEARNING_PHASE = 5 * fps                        # Initialization phase to estimate the reference frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new object should remain static for 21.2 frames i.e. 1.77 seconds\n"
     ]
    }
   ],
   "source": [
    "def create_mog2_background_subtractor():\n",
    "    mog2 = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "    # [ HISTORY ]\n",
    "    # Set's the number of last frames that affect the background model.\n",
    "    mog2.setHistory(LEARNING_PHASE)\n",
    "\n",
    "    # [ BACKGROUND RATIO ] \n",
    "    # Determines the portion of the history used to model the background, directly influencing how quickly new objects are integrated into it.\n",
    "    # A higher backgroundRatio results in a more stable model that is less sensitive to temporary changes, while a lower backgroundRatio allows for faster adaptation to new objects. \n",
    "    # If a foreground pixel maintains a nearly constant value for approximately backgroundRatio × history frames, it is reclassified as background and incorporated into the model as a \n",
    "    # new Gaussian component.\n",
    "    # cf is a measure of the maximum portion of the data that can belong to foreground objects without inﬂuencing the background model. \n",
    "    # If the object remains static long enough, its weight becomes larger than cf and it can be considered to be part of the background.\n",
    "    # For example, the choice of cf​=0.1 implies that the algorithm will retain 90% of the existing background model, making it more stable. \n",
    "    # (cf=0.1 --> background_ratio=0.9)\n",
    "    # A lower cf​ (higher Background Ratio) will make the model more stable, while a higher cf​ (lower Background Ratio) will allow for quicker adaptation.\n",
    "    mog2.setBackgroundRatio(70 / 100)             # cf=0.3 --> background_ratio=0.7\n",
    "    frames_of_stability = test_background_ratio(mog2.getBackgroundRatio(), mog2.getHistory())\n",
    "    print(f\"A new object should remain static for {frames_of_stability:.{3}} frames i.e. {(frames_of_stability / fps):.{3}} seconds\")\n",
    "\n",
    "    # [ SHADOWS ]\n",
    "    mog2.setDetectShadows(True)\n",
    "    mog2.setShadowThreshold(0.70)                 # A lower value may help in detecting more shadows.\n",
    "    mog2.setShadowValue(BLACK)                    # Detect shadows and hide them\n",
    "\n",
    "    # [ VARIANCES ]\n",
    "    # mog2.setVarInit(25)                         # default: 15\n",
    "    # mog2.setVarMax(5 * 25)                      # default: 5 * 15\n",
    "    # mog2.setVarMin(4)                           # default: 4\n",
    "\n",
    "    # [ THRESHOLDS ]\n",
    "    # The main threshold on the squared Mahalanobis distance to decide if the sample is \n",
    "    # well described by the background model or not. Related to Cthr from the paper. \n",
    "    mog2.setVarThreshold(4.5**2)                  # default: 16\n",
    "\n",
    "    # Threshold for the squared Mahalanobis distance that helps decide when a sample is close to the existing \n",
    "    # components (corresponds to Tg in the paper). If a pixel is not close to any component, it is considered\n",
    "    # foreground or added as a new component. 3 sigma => Tg=3*3=9 is default. A smaller Tg value generates \n",
    "    # more components. A higher Tg value may result in a small number of components but they can grow too large. \n",
    "    # mog2.setVarThresholdGen(3**2)               # default:  9\n",
    "    return mog2\n",
    "\n",
    "# SETUP - MOG2 Background Subtractor\n",
    "mog2 = create_mog2_background_subtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released Video Resource\n"
     ]
    }
   ],
   "source": [
    "# SETUP - Temporal Median Background\n",
    "frame_buffer = []\n",
    "frame_count = 0\n",
    "skip_count = 0\n",
    "temporalMedianBackground = None\n",
    " \n",
    "fgmask_diff = None\n",
    "\n",
    "if os.path.exists(file_output):\n",
    "    os.remove(file_output)\n",
    "\n",
    "# header of the output file\n",
    "header = [['Frame ID', 'Number of detected objects'], ['Object ID', 'Area', 'Perimeter', 'Classification', '[Phantom]']]\n",
    "with open(file_output, \"a\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(header)\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame_original = cap.read()\n",
    "    if not ret or frame_original is None:\n",
    "        cap.release()\n",
    "        print(\"Released Video Resource\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    skip_count += 1\n",
    "\n",
    "    frame = cv2.cvtColor(frame_original, cv2.COLOR_BGR2GRAY)\n",
    "    frame = preprocessing(frame, ProcessingType.DENOISE)\n",
    "\n",
    "    #     ------------------------------\n",
    "    # [1] |  TEMPORAL MEDIAN FILTER    |\n",
    "    #     ------------------------------\n",
    "    if len(frame_buffer) == 0 or skip_count == SKIP_FRAMES:\n",
    "        skip_count = 0\n",
    "        frame_buffer.append(frame)\n",
    "        if len(frame_buffer) > MAX_HISTORY:\n",
    "            frame_buffer.pop(0)\n",
    "\n",
    "        temporalMedianBackground = np.median(frame_buffer, axis=0).astype(dtype=np.uint8)\n",
    "        temporalMedianBackground_copy = temporalMedianBackground.copy()\n",
    "        cv2.putText(temporalMedianBackground_copy, f\"FRAME: {frame_count}/{total_frame_count}\", (5, 25), font, 0.5, (0, 0, 0), 1) \n",
    "        SHOW_GUI and cv2.imshow(\"temporalMedianBackground\", temporalMedianBackground_copy)\n",
    "\n",
    "    #     -------------------------------\n",
    "    # [2] |            MOG2             |\n",
    "    #     -------------------------------\n",
    "    learning_rate = compute_learning_rate(frame_count, mog2.getHistory(), dynamic=False)\n",
    "    mog2_fgmask = mog2.apply(frame, learning_rate)\n",
    "    mog2_fgmask_copy = mog2_fgmask.copy()\n",
    "    cv2.putText(mog2_fgmask_copy, f\"FRAME: {frame_count}/{total_frame_count}\", (5, 25), font, 0.5, 255, 1) \n",
    "    SHOW_GUI and cv2.imshow(\"mog2_fgmask\", mog2_fgmask_copy)\n",
    "\n",
    "    mog2_fgmask = postprocessing(mog2_fgmask, ProcessingType.MOG)\n",
    "    SHOW_GUI and cv2.imshow(\"mog2_fgmask postprocess\", mog2_fgmask)\n",
    "\n",
    "    if frame_count > LEARNING_PHASE:\n",
    "        #     -------------------------------\n",
    "        # [3] |            ROI              |\n",
    "        #     -------------------------------\n",
    "        found, roi = get_roi_foreground(mog2_fgmask)\n",
    "        \n",
    "        args = (STATIC_THRESHOLD_HYSTERESIS, STATIC_THRESHOLD)\n",
    "        tmb_fgmask = background_subtraction(frame, temporalMedianBackground, False, args)\n",
    "        SHOW_GUI and cv2.imshow(\"tmb_fgmask\", tmb_fgmask)\n",
    "        tmb_fgmask = postprocessing(tmb_fgmask, ProcessingType.TEMPORAL_MEDIAN)\n",
    "        SHOW_GUI and cv2.imshow(\"tmb_fgmask postprocess\", tmb_fgmask)\n",
    "        \n",
    "        if found:\n",
    "            combined_fgmask = mog2_fgmask.copy()\n",
    "            x_min, x_max, y_min, y_max = roi\n",
    "            # [ DRAW ROI ]\n",
    "            # frame_contours = frame.copy()\n",
    "            # cv2.rectangle(frame_contours, (x_min, y_min), (x_max, y_max), 255, 2)\n",
    "            # cv2.imshow(\"frame ROI\", frame_contours)\n",
    "\n",
    "            # mog2_fgmask_copy = mog2_fgmask.copy()\n",
    "            # cv2.rectangle(mog2_fgmask_copy, (x_min, y_min), (x_max, y_max), 127, 2)\n",
    "            # cv2.imshow(\"mog2 ROI\", mog2_fgmask_copy)\n",
    "\n",
    "            # tmb_fgmask_copy = tmb_fgmask.copy()\n",
    "            # cv2.rectangle(tmb_fgmask_copy, (x_min, y_min), (x_max, y_max), 127, 2)\n",
    "            # cv2.imshow(\"tmb ROI\", tmb_fgmask_copy)\n",
    "\n",
    "            #     -------------------------------\n",
    "            # [4] |    COMBINE FGMASK IN ROI    |\n",
    "            #     -------------------------------\n",
    "            combined_fgmask[y_min:y_max, x_min:x_max] = cv2.bitwise_or(mog2_fgmask[y_min:y_max, x_min:x_max], tmb_fgmask[y_min:y_max, x_min:x_max])    \n",
    "            combined_fgmask = postprocessing(combined_fgmask, ProcessingType.COMBINED)\n",
    "            SHOW_GUI and cv2.imshow(\"combined_fgmask postprocessing\", combined_fgmask)\n",
    "            \n",
    "            #     --------------------------------\n",
    "            # [5] |   STATIC OBJECT DETECTION    |\n",
    "            #     --------------------------------\n",
    "            fgmask_diff_new = cv2.bitwise_and(tmb_fgmask, cv2.bitwise_not(combined_fgmask))\n",
    "            if fgmask_diff is None:\n",
    "                fgmask_diff = fgmask_diff_new\n",
    "            \n",
    "            #     ---------------------------------\n",
    "            # [6] |   PRESERVE SEEN DIFFERENCES   |\n",
    "            #     ---------------------------------\n",
    "            fgmask_diff = cv2.bitwise_or(fgmask_diff, fgmask_diff_new)\n",
    "            # fgmask_diff = postprocessing(fgmask_diff, ProcessingType.DIFF)\n",
    "            SHOW_GUI and cv2.imshow('frame diff', fgmask_diff)\n",
    "        \n",
    "            complete_mask = cv2.bitwise_or(combined_fgmask, fgmask_diff)\n",
    "\n",
    "            # fill masks\n",
    "            complete_mask = process_contours(complete_mask)\n",
    "            fgmask_diff = process_contours(fgmask_diff)\n",
    "            combined_fgmask = process_contours(combined_fgmask)                \n",
    "\n",
    "            #     -----------------------------------\n",
    "            # [7] |  CONNECTED COMPONENTS ANALYSIS  |\n",
    "            #     -----------------------------------\n",
    "            output_diff = cv2.connectedComponentsWithStats(fgmask_diff, connectivity=8)\n",
    "            (num_labels_diff, labels_diff, stats_diff, centroids_diff) = output_diff\n",
    "            output_comb = cv2.connectedComponentsWithStats(combined_fgmask, connectivity=8)\n",
    "            (num_labels_comb, labels_comb, stats_comb, centroids_comb) = output_comb\n",
    "\n",
    "            output_complete = cv2.connectedComponentsWithStats(complete_mask, connectivity=8)\n",
    "            (num_labels_complete, labels_complete, stats_complete, centroids_complete) = output_complete\n",
    "            \n",
    "            labeled = imshow_components(labels_diff, labels_comb)\n",
    "            cv2.putText(labeled, f\"FRAME: {frame_count}/{total_frame_count}\", (5, 25), font, 0.5, (255, 255, 255), 1) \n",
    "            cv2.imshow(\"labeled\", labeled)\n",
    "\n",
    "            contours_fg, _ = cv2.findContours(complete_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "            frame_original_contours = frame_original.copy()\n",
    "            frame_original_filled = frame_original.copy()\n",
    "            frame_original_filled = cv2.drawContours(frame_original_filled, contours_fg, -1, (0,0,255), cv2.FILLED)\n",
    "            frame_original_contours = cv2.drawContours(frame_original_contours, contours_fg, -1, (0,255,0), 3)\n",
    "            SHOW_GUI and cv2.imshow(\"complete filled\", frame_original_filled)\n",
    "            SHOW_GUI and cv2.imshow('complete contours', frame_original_contours)\n",
    "\n",
    "            # results on text file\n",
    "            real_labels = num_labels_complete - 1 # -1 background\n",
    "            data = [[frame_count, real_labels]]\n",
    "            frame_original_rects = frame_original.copy()\n",
    "            for i in range(len(contours_fg)):\n",
    "                area = int(cv2.contourArea(contours_fg[i]))\n",
    "                perimeter = int(cv2.arcLength(contours_fg[i], True))\n",
    "                if area > 5000:\n",
    "                    type = 'person'\n",
    "                    data.append([i+1, area, perimeter, type])\n",
    "                else:\n",
    "                    if area < 100:\n",
    "                        continue\n",
    "\n",
    "                    type = 'other'\n",
    "                    \n",
    "                    phantom = find_phantoms(contours_fg[i], frame, temporalMedianBackground, fgmask_diff)\n",
    "                    if phantom == -1:\n",
    "                        continue\n",
    "                            \n",
    "                    data.append([i+1, area, perimeter, type, not(phantom)])\n",
    "            \n",
    "            SHOW_GUI and cv2.imshow(\"RECTS\", frame_original_rects)\n",
    "\n",
    "            with open(file_output, \"a\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerows(data)\n",
    "                        \n",
    "\n",
    "    cmd = cv2.waitKey(0)    \n",
    "    if cmd == ord(\"q\"):\n",
    "        break\n",
    "    if cmd == ord(\"n\"):\n",
    "        continue\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
