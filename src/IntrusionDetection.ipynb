{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.morphology import closing, square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"../video/rilevamento-intrusioni-video.wm\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "assert cap.isOpened(), \"Not opened!\"\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "total_frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "length = total_frame_count / fps\n",
    "\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(f\"[I] Video FPS: {fps}\")\n",
    "print(f\"[I] Video Total frame count: {total_frame_count}\")\n",
    "print(f\"[I] Video Length: {length}\")\n",
    "print(f\"[I] Video Frame Width: {width}\")\n",
    "print(f\"[I] Video Frame height: {height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to understand MOG2 parameters\n",
    "\n",
    "# # BackgroundRatio --> ok default (0.9) per ora quindi\n",
    "# If a foreground pixel keeps semi-constant value for about backgroundRatio*history frames, it's considered background and added to the model \n",
    "# as a center of a new component.\n",
    "# \"In altre parole, TB controlla quanto velocemente il modello di sfondo si adatta ai cambiamenti nella scena.\"\n",
    "# - Un valore più alto rende il modello di sfondo più adattabile e meno sensibile ai movimenti.\n",
    "# - Un valore più basso rende il modello di sfondo più stabile e più sensibile ai movimenti, ma anche più lento ad adattarsi.\n",
    "\n",
    "# valore simile a quelo di default --> lasciamo quello di default\n",
    "# print(\"Mean Background ratio: \", np.mean(ratios))\n",
    "\n",
    "# # VarThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "#     Performed a sorted insertion of pv into the frame buffer in the pi row.\n",
    "#     pv: pixel value\n",
    "#     pi: pixel index\n",
    "#     frame_buffer (global)\n",
    "# \"\"\"\n",
    "# def sortAndInsert(pv, pi, ):\n",
    "#     global frame_buffer\n",
    "#     insertion_idx = np.searchsorted(frame_buffer[pi], pv)\n",
    "#     if insertion_idx == len(frame_buffer[pi]):\n",
    "#         insertion_idx =- 1\n",
    "\n",
    "#     frame_buffer[pi, insertion_idx] = pv\n",
    "\n",
    "#cv2.imshow(\"frame_copy\", frame_contours)\n",
    "# largestContour = max(contours, key = cv2.contourArea) # get largest contour\n",
    "# rect = cv2.minAreaRect(largestContour)\n",
    "# box = np.int64(cv2.boxPoints(rect))\n",
    "# cv2.drawContours(frame_contours, [box], 0, (0,0,255), 1)\n",
    "# cv2.imshow(\"frame_contours\", frame_contours)\n",
    "\n",
    "# cv2.imshow(f\"temporalMedianBackground-frame-{frame_count}\", temporalMedianBackground)\n",
    "# cmd = cv2.waitKey(0)\n",
    "# cv2.destroyWindow(f\"temporalMedianBackground-frame-{frame_count}\")\n",
    "# if cmd == ord(\"q\"):\n",
    "#     break\n",
    "# if cmd == ord(\"n\"):\n",
    "#     continue\n",
    "\n",
    "# alpha = 3.0 # Contrast control\n",
    "# beta = 25 # Brightness control\n",
    "# # call convertScaleAbs function\n",
    "# #adjusted = cv2.convertScaleAbs(mog2_fgmask, alpha=alpha, beta=beta)\n",
    "\n",
    "\n",
    "# plt.hist(frame_diff.ravel(), 256, [0, 256])\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "            \n",
    "# Disegna la bounding box\n",
    "#cv2.rectangle(frame_contours, (x_min, y_min), (x_max, y_max), 255, 2)\n",
    "\n",
    "#cv2.imshow(\"frame_contours\", frame_contours)\n",
    "\n",
    "\n",
    "# analysis = cv2.connectedComponentsWithStats(tmb_fgmask, 8, cv2.CV_32S)\n",
    "# (totalLabels, label_ids, values, centroid) = analysis \n",
    "\n",
    "# # Initialize a new image to store  \n",
    "# # all the output components \n",
    "# output = np.zeros(tmb_fgmask.shape, dtype=\"uint8\") \n",
    "\n",
    "# # Loop through each component \n",
    "# for i in range(1, totalLabels): \n",
    "    \n",
    "#     # Area of the component \n",
    "#     area = values[i, cv2.CC_STAT_AREA]  \n",
    "    \n",
    "#     if (area > 140) and (area < 400): \n",
    "#         componentMask = (label_ids == i).astype(\"uint8\") * 255\n",
    "#         output = cv2.bitwise_or(output, componentMask) \n",
    "\n",
    "# cv2.imshow(\"Filtered Components\", output)\n",
    "# \n",
    "# \n",
    "#mog2 = cv2.createBackgroundSubtractorMOG2(history=fps*2, varThreshold=20, detectShadows=False)\n",
    "\n",
    "# print(mog2.getHistory())\n",
    "# print(mog2.getVarThreshold())\n",
    "# print(mog2.getBackgroundRatio())\n",
    "# print(mog2.getNMixtures())\n",
    "# print(mog2.getVarThresholdGen())\n",
    "# print(mog2.getVarMin())\n",
    "# print(mog2.getVarMax())\n",
    "# print(mog2.getComplexityReductionThreshold())\n",
    "# print(mog2.getVarInit())\n",
    "\n",
    "#mog2.setComplexityReductionThreshold(0)\n",
    "#mog2.setBackgroundRatio(0.7)\n",
    "#mog2.setNMixtures(1)\n",
    "#mog2.setVarThresholdGen(30) \n",
    "\n",
    "#temporalMedianBackground_tmp = np.median(frames_tmp, axis=0).astype(dtype=np.uint8)\n",
    "#cv2.imshow(\"temporal background - true one\", temporalMedianBackground)\n",
    "# make addWeighted between temporalMedianBackground and frame just where the background_mask is 1\n",
    "#masked_img = np.where(background_mask[..., None] == 255, frame[..., None], 0) # dove c'è il background mask metti frame, altrimenti 0\n",
    "#cv2.imshow(\"masked_img\", masked_img)\n",
    "#temporalMedianBackground = cv2.addWeighted(temporalMedianBackground, 0.7, masked_img, 0.3, 0)\n",
    "#temporalMedianBackground = cv2.bitwise_and(temporalMedianBackground, temporalMedianBackground, background_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(frame, type):\n",
    "    \"\"\"\n",
    "        Apply all the preprocessing steps to a copy of the passed frame.\n",
    "\n",
    "        Arguments:\n",
    "            frame (MatLike): frame to process\n",
    "            type (ProcessingType): which type of preprocess\n",
    "\n",
    "        Returns:\n",
    "            (MatLike): a processed copy of the frame\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(type, ProcessingType), \"type must be an instance of ProcessingType (Enum)\"\n",
    "    output = frame.copy()\n",
    "    \n",
    "    if type == ProcessingType.DENOISE:\n",
    "        # output = cv2.medianBlur(output, 5)\n",
    "        output = cv2.GaussianBlur(output, (5, 5), 2)\n",
    "        \n",
    "    elif type == ProcessingType.EDGES:\n",
    "        # Laplacian sharpening\n",
    "        # laplacian = cv2.Laplacian(output)\n",
    "        # laplacian = cv2.convertScaleAbs(output)\n",
    "        # output = cv2.add(output, -laplacian)\n",
    "    \n",
    "        sharpening = np.array([ [0 , -1,  0],\n",
    "                                [-1,  5, -1],\n",
    "                                [0 , -1,  0]])\n",
    "        output = cv2.filter2D(output, -1, sharpening)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"ProcessingType.type not found\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_learning_rate(frame_count, history_length, dynamic = True, bias = 0):\n",
    "    \"\"\"\n",
    "        Computes the learning rate\n",
    "\n",
    "        Arguments:\n",
    "            frame_count (int) : current frame number\n",
    "            history_lenght (int): number of frames in buffer\n",
    "            dynamic (boolean)\n",
    "            bias (float): constant added to the computed learning rate\n",
    "\n",
    "        Returns:\n",
    "            float: the learning rate to use\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    assert frame_count > 0, \"Frame Count must be greater than zero\"\n",
    "    if dynamic and frame_count < history_length:\n",
    "        return (1 / frame_count) + bias\n",
    "    \n",
    "    return (1 / history_length) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import flood_fill\n",
    "def postprocessing(fgmask, type):\n",
    "    \"\"\"\n",
    "        Apply a pipeline of morphological operators to improve the segmentation\n",
    "    \"\"\"\n",
    "    assert isinstance(type, ProcessingType), \"type must be an instance of ProcessingType (Enum)\"\n",
    "    # cv2.imshow(\"fgmask UNPROCESSED\", fgmask)\n",
    "    output = fgmask.copy()\n",
    "    element = np.ones((3,3), np.uint8)\n",
    "\n",
    "    if type == ProcessingType.MOG:\n",
    "        output = cv2.medianBlur(output, 3)\n",
    "        # output = cv2.morphologyEx(output, cv2.MORPH_CLOSE, element, iterations=1)\n",
    "        \n",
    "        # output = flood_fill(output, (1,1), 127, connectivity=25)\n",
    "    else:\n",
    "        raise ValueError(\"ProcessingType.type not found\")\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import ndimage as ndi\n",
    "# from skimage import morphology\n",
    "# def apply_hysteresis_threshold(image, low, high):\n",
    "#     \"\"\"Apply hysteresis thresholding to ``image``.\n",
    "\n",
    "#     This algorithm finds regions where ``image`` is greater than ``high``\n",
    "#     OR ``image`` is greater than ``low`` *and* that region is connected to\n",
    "#     a region greater than ``high``.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     image : (M[, ...]) ndarray\n",
    "#         Grayscale input image.\n",
    "#     low : float, or array of same shape as ``image``\n",
    "#         Lower threshold.\n",
    "#     high : float, or array of same shape as ``image``\n",
    "#         Higher threshold.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     thresholded : (M[, ...]) array of bool\n",
    "#         Array in which ``True`` indicates the locations where ``image``\n",
    "#         was above the hysteresis threshold.\n",
    "\n",
    "#     Examples\n",
    "#     --------\n",
    "#     >>> image = np.array([1, 2, 3, 2, 1, 2, 1, 3, 2])\n",
    "#     >>> apply_hysteresis_threshold(image, 1.5, 2.5).astype(int)\n",
    "#     array([0, 1, 1, 1, 0, 0, 0, 1, 1])\n",
    "\n",
    "#     References\n",
    "#     ----------\n",
    "#     .. [1] J. Canny. A computational approach to edge detection.\n",
    "#            IEEE Transactions on Pattern Analysis and Machine Intelligence.\n",
    "#            1986; vol. 8, pp.679-698.\n",
    "#            :DOI:`10.1109/TPAMI.1986.4767851`\n",
    "#     \"\"\"\n",
    "#     low = np.clip(low, a_min=None, a_max=high)  # ensure low always below high\n",
    "#     mask_low = image > low\n",
    "#     mask_high = image > high\n",
    "\n",
    "#     # remove small objects\n",
    "#     processed = morphology.remove_small_objects(mask_high.astype(bool), min_size=25, connectivity=5).astype(int)\n",
    "\n",
    "#     # black out pixels\n",
    "#     mask_x, mask_y = np.where(processed == 0)\n",
    "#     mask_high[mask_x, mask_y] = 0\n",
    "\n",
    "#     # Connected components of mask_low\n",
    "#     labels_low, num_labels = ndi.label(mask_low)\n",
    "#     # Check which connected components contain pixels from mask_high\n",
    "#     sums = ndi.sum(mask_high, labels_low, np.arange(num_labels + 1))\n",
    "#     connected_to_high = sums > 0\n",
    "#     thresholded = connected_to_high[labels_low]\n",
    "#     return thresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology, img_as_ubyte, filters\n",
    "def background_subtraction(frame, background, adaptive, args):\n",
    "    \"\"\"\n",
    "        Computes the absolute difference between the frame and the background then applies a threshold to segment the\n",
    "        background and the foreground.\n",
    "\n",
    "        Arguments:\n",
    "            frame (Mat): image, current frame\n",
    "            background (Mat): image, estimated reference frame\n",
    "            args:\n",
    "             - threshold (Int)=25: if defined, is the value of the static threshold\n",
    "             - C (float)=None: if defined, it will use and adaptive thresholding method and it represent the value of C (Constant subtracted from the mean or weighted mean)\n",
    "            \n",
    "        Returns:\n",
    "            fgmask: binary image with the foreground white (255).\n",
    "    \"\"\"\n",
    "    diff = cv2.absdiff(frame, background)\n",
    "    # diff_edges = diff_canny_edges(frame, background)\n",
    "    \n",
    "    # cv2.imshow(\"diff\", diff)\n",
    "    # cv2.imshow(\"diff_edges\", diff_edges)\n",
    "\n",
    "    if adaptive:\n",
    "        (C, block_size) = args\n",
    "        fgmask = cv2.adaptiveThreshold(diff, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, block_size, C)\n",
    "    else:\n",
    "        (t_low, t_high) = args\n",
    "        hysteresis = filters.apply_hysteresis_threshold(diff, t_low, t_high)\n",
    "        # TODO: remove edge to create a black contour\n",
    "        fgmask = img_as_ubyte(hysteresis)\n",
    "\n",
    "        # fgmask = combine_fgmask_with_edges(fgmask, diff_edges)\n",
    "\n",
    "    return fgmask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_roi_foreground(fgmask):\n",
    "#     enclosed_mask = morphology.convex_hull_image(fgmask)\n",
    "#     enclosed_mask = img_as_ubyte(enclosed_mask)\n",
    "#     # cv2.imshow(\"enclosed_mask\", enclosed_mask)\n",
    "\n",
    "#     enclosed_mask_copy = fgmask.copy()\n",
    "#     enclosed_mask_copy = cv2.cvtColor(enclosed_mask_copy, cv2.COLOR_GRAY2BGR)\n",
    "#     contours, _ = cv2.findContours(enclosed_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "#     if len(contours) > 0:\n",
    "#         largestContour = max(contours, key = cv2.contourArea) # get largest contour\n",
    "#         rect = cv2.minAreaRect(largestContour)\n",
    "#         box  = np.int64(cv2.boxPoints(rect))\n",
    "#         cv2.drawContours(enclosed_mask_copy, [box], 0, (0,0,255), 4)\n",
    "#         cv2.imshow(\"enclosed_mask contours\", enclosed_mask_copy)\n",
    "#     return enclosed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roi_foreground(fgmask):\n",
    "    blur = cv2.GaussianBlur(fgmask, (5,5), 3)\n",
    "    thresh = threshold_otsu(blur)\n",
    "    if thresh == 0:\n",
    "        return False, None\n",
    "    \n",
    "    bw = closing(blur >= thresh, square(7))\n",
    "    #erode = cv2.erode(bw.astype(np.uint8) * 255, None, iterations=3)\n",
    "    #cv2.imshow(\"erode\", erode)\n",
    "    erode = cv2.medianBlur(bw.astype(np.uint8) * 255, 3)\n",
    "    dilated = cv2.dilate(erode, None, iterations=5)\n",
    "    #cv2.imshow(\"dilate\", dilated)        \n",
    "\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    real_countours = []\n",
    "    for contour in contours:\n",
    "        # (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        if cv2.contourArea(contour) > 100:\n",
    "            real_countours.append(contour)\n",
    "        \n",
    "    \n",
    "    if len(real_countours) == 0:\n",
    "        return False, None\n",
    "    \n",
    "    # Define the ROI \n",
    "    x_min = min([np.min(cnt[:, :, 0]) for cnt in real_countours])\n",
    "    x_max = max([np.max(cnt[:, :, 0]) for cnt in real_countours])\n",
    "    y_min = min([np.min(cnt[:, :, 1]) for cnt in real_countours])\n",
    "    y_max = max([np.max(cnt[:, :, 1]) for cnt in real_countours])\n",
    "\n",
    "    padding = 10\n",
    "    x_min = max(x_min - padding, 0)\n",
    "    x_max = min(x_max + padding, 320)\n",
    "    y_min = max(y_min - padding, 0)\n",
    "    y_max = min(y_max + padding, 240)\n",
    "\n",
    "    return True, (x_min, x_max, y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_operator(img, r):\n",
    "    exp_img = ((img/255)**r) * 255\n",
    "    return exp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_canny_edges(frame, background):\n",
    "    \"\"\"\n",
    "        Computes the difference of the Canny's egdes of the current frame and Canny's egdes of the background.\n",
    "    \n",
    "        Arguments:\n",
    "            frame (Mat): current frame\n",
    "            background (Mat): estimated reference frame\n",
    "            \n",
    "        Returns:\n",
    "            img: binary image with differences of edges\n",
    "    \"\"\"\n",
    "    upthresh = 150\n",
    "    lothresh = upthresh // 2\n",
    "\n",
    "    background = preprocessing(background, ProcessingType.EDGES)\n",
    "    edge_background = cv2.Canny(background, lothresh, upthresh)\n",
    "    \n",
    "    # enlarge background edges to better cancel out with the ones of the frame\n",
    "    # elem = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n",
    "    # edge_background = cv2.dilate(edge_background, elem, iterations=1)\n",
    "    \n",
    "    frame = preprocessing(frame, ProcessingType.EDGES)\n",
    "    edge_frame = cv2.Canny(frame, lothresh, upthresh)\n",
    "\n",
    "    # edge_diff will contain the edges that are present in the frame but not in the background\n",
    "    # frame  back      output\n",
    "    #     0     0 -->       0\n",
    "    #   255     0 -->     255\n",
    "    #     0   255 -->       0      (0 - 255 = -255 --> 0)\n",
    "    #   255   255 -->       0\n",
    "    edge_diff = edge_frame - np.min([edge_frame, edge_background], axis=0)\n",
    "    # cv2.imshow(\"edge_diff\", edge_diff)\n",
    "\n",
    "    processed = morphology.remove_small_objects(edge_diff.astype(bool), min_size=6, connectivity=5).astype(int)\n",
    "\n",
    "    # black out pixels\n",
    "    mask_x, mask_y = np.where(processed == 0)\n",
    "    edge_diff[mask_x, mask_y] = 0\n",
    "    \n",
    "    return edge_diff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ CONNECTED COMPONENT ANALYSIS ]\n",
    "\n",
    "# loop over the number of unique connected component labels\n",
    "# for i in range(0, numLabels):\n",
    "#     # if this is the first component then we examine the\n",
    "#     # *background* (typically we would just ignore this\n",
    "#     # component in our loop)\n",
    "#     if i == 0:\n",
    "#         text = f\"examining component {i+1}/{numLabels} (background)\"\n",
    "#     # otherwise, we are examining an actual connected component\n",
    "#     else:\n",
    "#         text = f\"examining component {i+1}/{numLabels}\"\n",
    "#     # print a status message update for the current connected\n",
    "#     # component\n",
    "#     print(\"[INFO] {}\".format(text))\n",
    "#     # extract the connected component statistics and centroid for\n",
    "#     # the current label\n",
    "#     x = stats[i, cv2.CC_STAT_LEFT]\n",
    "#     y = stats[i, cv2.CC_STAT_TOP]\n",
    "#     w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "#     h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "#     area = stats[i, cv2.CC_STAT_AREA]\n",
    "#     (cX, cY) = centroids[i]\n",
    "\n",
    "#     # clone our original image (so we can draw on it) and then draw\n",
    "#     # a bounding box surrounding the connected component along with\n",
    "#     # a circle corresponding to the centroid\n",
    "#     output = image.copy()\n",
    "#     cv2.rectangle(output, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "#     cv2.circle(output, (int(cX), int(cY)), 4, (0, 0, 255), -1)    \n",
    "\n",
    "#     # construct a mask for the current connected component by\n",
    "#     # finding a pixels in the labels array that have the current\n",
    "#     # connected component ID\n",
    "#     componentMask = (labels == i).astype(\"uint8\") * 255\n",
    "#     # show our output image and connected component mask\n",
    "#     cv2.imshow(\"Output\", output)\n",
    "#     cv2.imshow(\"Connected Component\", componentMask)\n",
    "#     cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_components(labels):\n",
    "    # Map component labels to hue val\n",
    "    label_hue = np.uint8(179*labels/np.max(labels))\n",
    "    blank_ch = 255*np.ones_like(label_hue)\n",
    "    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
    "\n",
    "    # cvt to BGR for display\n",
    "    labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # set bg label to black\n",
    "    labeled_img[label_hue==0] = 0\n",
    "\n",
    "    # cv2.imshow('labeled.png', labeled_img)\n",
    "    return labeled_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "# class syntax\n",
    "class ProcessingType(Enum):\n",
    "    DENOISE = 1\n",
    "    BACKGROUND_FRAME = 2\n",
    "    EDGES = 3\n",
    "    MOG = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_fgmask_with_edges(fgmask, edges):\n",
    "#     output = cv2.bitwise_or(fgmask, edges)\n",
    "#     cv2.imshow(\"without Canny Edges\", fgmask)\n",
    "    \n",
    "#     # Combine edges\n",
    "#     output = cv2.bitwise_or(fgmask, edges)\n",
    "#     cv2.imshow(\"with Canny Edges\", output)\n",
    "\n",
    "#     # Process image\n",
    "#     kernel = np.ones((8,8) , np.uint8)\n",
    "#     output = cv2.morphologyEx(output, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "#     cv2.imshow(\"test\", output)\n",
    "\n",
    "#     # Remove the same added egdes\n",
    "#     dilate_edges = cv2.dilate(edges, kernel, iterations=2)\n",
    "#     dilate_edges = cv2.morphologyEx(dilate_edges, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "#     found, (x_min, x_max, y_min, y_max) = getRectangularROI(fgmask)\n",
    "#     mask = np.zeros_like(output)\n",
    "#     mask[x_min:x_max, y_min:y_max] = 1\n",
    "\n",
    "#     output = cv2.bitwise_and(output, cv2.bitwise_not(dilate_edges), mask)\n",
    "#     cv2.imshow(\"test 2\", output)\n",
    "\n",
    "#     # Close the small gaps created\n",
    "#     kernel = np.ones((3,3) , np.uint8)\n",
    "#     output = cv2.morphologyEx(output, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "\n",
    "#     cv2.imshow(\"test 3\", output)\n",
    "#     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMTERS\n",
    "FRAME_BUFFERED_PER_SECOND = 2               # 2 images added each second to frame buffer\n",
    "MAX_HISTORY = 3 * fps                       # 2x12 frames stored in the \"circular buffer\"\n",
    "SKIP_FRAMES = fps // FRAME_BUFFERED_PER_SECOND\n",
    "\n",
    "STATIC_THRESHOLD = 35\n",
    "STATIC_THRESHOLD_HYSTERESIS = 25\n",
    "\n",
    "LEARNING_PHASE = 5 * fps                    # Initialization phase for the background subtractor to estimate the reference frame\n",
    "\n",
    "# MOG2 Background Subtractor\n",
    "mog2 = cv2.createBackgroundSubtractorMOG2()\n",
    "mog2.setHistory(LEARNING_PHASE)\n",
    "mog2.setBackgroundRatio(0.70)\n",
    "\n",
    "BLACK = 0\n",
    "mog2.setDetectShadows(True)\n",
    "mog2.setShadowValue(BLACK)                  # Detect shadows and hide them\n",
    "mog2.setShadowThreshold(0.75)\n",
    "\n",
    "# mog2.setVarInit(5)                          # default: 15\n",
    "# mog2.setVarMax(25)                          # default: 5 * 15\n",
    "# mog2.setVarMin(4)                           # default: 4\n",
    "\n",
    "# The main threshold on the squared Mahalanobis distance to decide if the sample is \n",
    "# well described by the background model or not. Related to Cthr from the paper. \n",
    "# mog2.setVarThreshold(5**2)                  # default: 16\n",
    "\n",
    "# Threshold for the squared Mahalanobis distance that helps decide when a sample is close to the existing \n",
    "# components (corresponds to Tg in the paper). If a pixel is not close to any component, it is considered\n",
    "# foreground or added as a new component. 3 sigma => Tg=3*3=9 is default. A smaller Tg value generates \n",
    "# more components. A higher Tg value may result in a small number of components but they can grow too large. \n",
    "# mog2.setVarThresholdGen(4**2)               # default:  9\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "frame_buffer = []\n",
    "frame_count = 0\n",
    "skip_count = 0\n",
    "\n",
    "temporalMedianBackground = None\n",
    "fgmask_diff = None\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "kernel = np.ones((3,3),np.uint8) # combined mask\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame_original = cap.read()\n",
    "    if not ret or frame_original is None:\n",
    "        cap.release()\n",
    "        print(\"Released Video Resource\")\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    skip_count += 1\n",
    "\n",
    "    frame = cv2.cvtColor(frame_original, cv2.COLOR_BGR2GRAY)\n",
    "    frame = preprocessing(frame, ProcessingType.DENOISE)\n",
    "\n",
    "    # cv2.imshow(\"current frame\", frame)\n",
    "\n",
    "    # -------------------------------\n",
    "    # |   TEMPORAL MEDIAN FILTER    |\n",
    "    # -------------------------------\n",
    "    # TODO: port np.ma.MaskedArray and np.ma.median to create a selective updated temporal median\n",
    "    if len(frame_buffer) == 0 or skip_count == SKIP_FRAMES:\n",
    "        skip_count = 0\n",
    "        frame_buffer.append(frame)\n",
    "\n",
    "        if len(frame_buffer) > MAX_HISTORY:\n",
    "            frame_buffer.pop(0)\n",
    "\n",
    "        temporalMedianBackground = np.median(frame_buffer, axis=0).astype(dtype=np.uint8)\n",
    "        temporalMedianBackground_copy = temporalMedianBackground.copy()\n",
    "        cv2.putText(temporalMedianBackground_copy, f\"FRAME: {frame_count}/{total_frame_count}\", (5, 25), font, 0.5, (0, 0, 0), 1) \n",
    "        cv2.imshow(\"temporalMedianBackground\", temporalMedianBackground_copy)\n",
    "\n",
    "    # -------------------------------\n",
    "    # |            MOG2             |\n",
    "    # -------------------------------\n",
    "    learning_rate = compute_learning_rate(frame_count, mog2.getHistory(), dynamic=False)\n",
    "    \n",
    "    mog2_fgmask = mog2.apply(frame, learning_rate)\n",
    "    mog2_fgmask = postprocessing(mog2_fgmask, ProcessingType.MOG)\n",
    "    # cv2.imshow(\"mog2_fgmask\", mog2_fgmask)\n",
    "    \n",
    "    # fgmask = combine_fgmask_with_edges(mog2_fgmask, edge_diff)\n",
    "    \n",
    "    if frame_count > LEARNING_PHASE:\n",
    "        # [ GET A ROI OF THE MOG2 FOREGROUND MASK ]\n",
    "        found, roi = get_roi_foreground(mog2_fgmask)\n",
    "        \n",
    "        args = (STATIC_THRESHOLD_HYSTERESIS, STATIC_THRESHOLD)\n",
    "        tmb_fgmask = background_subtraction(frame, temporalMedianBackground, False, args)\n",
    "        # cv2.imshow(\"tmb_fgmask\", tmb_fgmask)\n",
    "        \n",
    "        # [ COMBINE FGMSKS IN ROI ]\n",
    "        if found:\n",
    "            combined_fgmask = mog2_fgmask.copy()\n",
    "            x_min, x_max, y_min, y_max = roi\n",
    "\n",
    "            # draw ROI\n",
    "            frame_contours = frame.copy()\n",
    "            cv2.rectangle(frame_contours, (x_min, y_min), (x_max, y_max), 255, 2)\n",
    "            cv2.imshow(\"frame ROI\", frame_contours)\n",
    "\n",
    "            mog2_fgmask_copy = mog2_fgmask.copy()\n",
    "            cv2.rectangle(mog2_fgmask_copy, (x_min, y_min), (x_max, y_max), 127, 2)\n",
    "            cv2.imshow(\"mog2 ROI\", mog2_fgmask_copy)\n",
    "\n",
    "            tmb_fgmask_copy = tmb_fgmask.copy()\n",
    "            cv2.rectangle(tmb_fgmask_copy, (x_min, y_min), (x_max, y_max), 127, 2)\n",
    "            cv2.imshow(\"tmb ROI\", tmb_fgmask_copy)\n",
    "\n",
    "            combined_fgmask[y_min:y_max, x_min:x_max] = cv2.bitwise_or(mog2_fgmask[y_min:y_max, x_min:x_max], tmb_fgmask[y_min:y_max, x_min:x_max])    \n",
    "            combined_fgmask = cv2.morphologyEx(combined_fgmask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "            # cv2.imshow(\"combined_fgmask\", combined_fgmask)\n",
    "\n",
    "            # [ OVERRIDE FROZEN VALUES ]\n",
    "            if fgmask_diff is not None:\n",
    "                complete = cv2.bitwise_or(combined_fgmask, fgmask_diff)\n",
    "\n",
    "                complete = cv2.dilate(complete, kernel, iterations=1)\n",
    "                complete = cv2.morphologyEx(complete, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "                cv2.imshow(\"combinedMask + diff\", complete)\n",
    "\n",
    "                # contours_fg, _ = cv2.findContours(complete, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "                \n",
    "                # frame_original_contours = cv2.drawContours(frame_original, contours_fg, -1, (0,0,255), cv2.FILLED)\n",
    "                # cv2.imshow(\"frame\", frame_original_contours)\n",
    "\n",
    "                output = cv2.connectedComponentsWithStats(complete, connectivity=8)\n",
    "                (numLabels, labels, stats, centroids) = output\n",
    "                # labels is a mask with an id for each pixel\n",
    "                filtered = labels.copy()\n",
    "                for i in range(0, numLabels):\n",
    "                    if stats[i, cv2.CC_STAT_AREA] < 200:\n",
    "                        filtered[filtered == i] = 0\n",
    "                \n",
    "                labeled = imshow_components(filtered)\n",
    "                cv2.imshow(\"labeled\", labeled)\n",
    "                # labeled = cv2.cvtColor(labeled, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # contours_fg, _ = cv2.findContours(labeled, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "                # frame_original_contours = cv2.drawContours(frame_original, contours_fg, -1, (0,0,255), cv2.FILLED)\n",
    "                # cv2.imshow(\"frame\", frame_original_contours)\n",
    "\n",
    "\n",
    "            # [ STATIC OBJECTS ]\n",
    "            fgmask_diff_new = cv2.bitwise_and(tmb_fgmask, cv2.bitwise_not(combined_fgmask))\n",
    "            if fgmask_diff is None:\n",
    "                fgmask_diff = fgmask_diff_new\n",
    "            \n",
    "            fgmask_diff = cv2.bitwise_or(fgmask_diff, fgmask_diff_new)\n",
    "            #fgmask_diff = cv2.medianBlur(fgmask_diff, 3)\n",
    "            #fgmask_diff = cv2.dilate(fgmask_diff, None, iterations=3)\n",
    "            cv2.imshow(\"diff (= BOOKS) = (median - combined)\", fgmask_diff)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    cmd = cv2.waitKey(0)    \n",
    "    if cmd == ord(\"q\"):\n",
    "        break\n",
    "    if cmd == ord(\"n\"):\n",
    "        continue\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"[I] Elapsed time: {end - start} s\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
